# -*- coding: utf-8 -*-
"""Assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EurpvCv8-SIM3BWVvDMmkhUEpIyxCox-
"""

#plotting and pandas
import pandas as pd
import numpy as np
import matplotlib.pylab as plt
from matplotlib.cm import ScalarMappable
import seaborn as sns

#sklearn
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold,cross_validate, ShuffleSplit, KFold
from sklearn import metrics
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score,roc_curve, f1_score, precision_score
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
from sklearn.tree import plot_tree
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis,QuadraticDiscriminantAnalysis
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier as GBC
from sklearn.ensemble import VotingClassifier as VoteC
from sklearn.svm import SVC
from sklearn import svm
from sklearn.decomposition import PCA

"""# Data Understanding"""

file = open("customer_datadictionary.txt", "r")
content = file.read()
print(content)
file.close()

df = pd.read_csv('customer.csv')
df

df.rename({'Unnamed: 0':'idx'}, axis=1, inplace=True)
df.set_index("idx", inplace = True)
df.head()

pd.set_option('display.max_colwidth', None)
# Define the data for the table as a dictionary
data = {
    "Variable": [
        "custid", "sex", "is_employed", "income", "marital_status", "health_ins",
        "housing_type", "num_vehicles", "age", "state_of_res", "code_column",
        "gas_usage", "rooms", "recent_move_b"
    ],
    "Type": [
        "Nominal", "Nominal", "Logical", "Ratio", "Nominal", "Logical",
        "Nominal", "Ratio/Ordinal", "Ratio", "Nominal", "Numeric",
        "Ratio", "Ordinal", "Logical"
    ],
    "Description": [
        "Unique identifier for each customer", "Gender of the customer (Male/Female)",
        "Employment status (TRUE/FALSE/NA for not in workforce)", "Annual income, with special codes for losses and rounded values",
        "Marital status of the customer", "Whether the customer has health insurance (TRUE/FALSE)",
        "Housing situation (e.g., rented, owned)", "Number of vehicles in the household (0–6, with 6+ as 6, NA for unknown)",
        "Age of the customer (0–150 years, with 0 as unknown)", "State of residence (51 values)",
        "Special numerical codes, likely for some the state_of_res",
        "Monthly gas bill amount (various codes for non-applicability)",
        "Number of rooms in the house (numeric)",
        "If the customer moved in the past year (TRUE/FALSE/unknown)"
    ],
    "Notes": [
        "Not useful for modeling; drop this column.", "Use for group-based analysis and one-hot encoding.",
        "Treat `NA` as a separate category or impute with domain knowledge.",
        "Requires cleaning, converting codes (e.g., '-000001') to meaningful numbers.",
        "Requires one-hot encoding for analysis and modeling.",
        "Target variable for modeling.",
        "Analyze relationship with income and marital status; encode nominal categories.",
        "Consider ordinal analysis; treat `NA` as separate or impute with median/mode.",
        "Impute `0` values or treat as a separate category for young/unknown customers.",
        "Requires grouping or dummy encoding; analyze region-based patterns if relevant.",
        "Clarify meaning before using. Consider dropping or transforming as necessary.",
        "Requires cleaning; encode 'NA' and transform for consistent scaling.",
        "Use as a feature for analysis or prediction.",
        "Encode `unknown` as a separate category; possible interaction with `housing_type`."
    ]
}

# Create a pandas DataFrame from the dictionary
df_table = pd.DataFrame(data)

# Display the table
df_table

print('Overview of the dataset','\n')
print("Rows:", df.shape[0])
print("\nNumber of features:", df.shape[1])
print("\nFeatures:",df.columns.tolist())
print("\nMissing values:", df.isnull().sum().values.sum())
print("\nUnique values:", df.nunique())

"""custid: string
sex: string
is_employed: logical
income: numeric
marital_status: string
health_ins: logical
housing_type: string
num_vehicles: numeric
age: numeric
state_of_res: string
code_column: numerical
gas_usage: numeric
rooms: int64
recent_move_b: logical
"""

df.dtypes

print("Number of missing values: \n",df.isnull().sum())

print("Some statistics describing the variables.\n")
df.describe()

pd.set_option('display.max_colwidth', None)

# Data on missing counts
data = {
    'Missing Count': df.isnull().sum(), '% Missing': np.zeros(14),
    'Proposed Handling': [
        "Drop column (ID not useful for analysis)",
        "No action needed",
        "Treat missing as a separate category or impute based on age/income patterns",
        "No action needed, consider transformations",
        "No action needed",
        "No action needed (target variable)",
        "Treat missing as 'unknown' category; consider imputation for modeling",
        "Impute with mode or median",
        "No action needed; handle '0' as special case",
        "No action needed; consider grouping by region",
        "Analyze relevance; drop if not informative",
        "Impute with median or treat missing as 'unknown'",
        "No action needed; explore relationship with other features",
        "Treat missing as 'unknown'; imputation not recommended without context"
    ]
}

# Create the DataFrame
df_summary = pd.DataFrame(data)
df_summary['% Missing'] = (df_summary['Missing Count'] / 72458) * 100

# Print the table
df_summary

# Reset all pandas display options to default
pd.reset_option('all')

import seaborn as sns
import matplotlib.pyplot as plt

# Replace missing values in 'is_employed' temporarily for grouping
df['is_employed_filled'] = df['is_employed'].fillna('Unknown')

# Group by employment status
grouped = df.groupby('is_employed_filled')[['age', 'income']].mean()
print(grouped)

# Plot patterns with age
sns.boxplot(x='is_employed_filled', y='age', data=df)
plt.title('Age Distribution by Employment Status')
plt.show()

# Plot patterns with income
sns.boxplot(x='is_employed_filled', y='income', data=df)
plt.title('Income Distribution by Employment Status')
plt.show()

"""Potential Imputation Strategy for Missing Employment Data:

Individuals with missing is_employed and high age (e.g., 60+) could be labeled as "Not in workforce."

Income could serve as another predictor. Low or zero income in the "Unknown" group likely corresponds to non-employment.
"""

# Impute missing is_employed based on income pattern
df['is_employed'] = df.apply(
    lambda row: 'Not in workforce' if pd.isna(row['is_employed']) and row['income'] == 0 else row['is_employed'],
    axis=1
)

print("Number of missing values: \n",df.isnull().sum())

df[(df['is_employed'].isna()) & (df['age'] >= 67)]

# Impute missing is_employed based on age pattern
df['is_employed'] = df.apply(
    lambda row: 'Not in workforce' if pd.isna(row['is_employed']) and row['age'] >= 67 else row['is_employed'],
    axis=1
)

print("Number of missing values: \n",df.isnull().sum())

"""Since I have no other way to check the NaN values, I will drop them"""

print("Total number of rows in data: ", len(df))
df = df.dropna(subset=['is_employed'])
print("Number of rows after removing nan: ",len(df))

print("Number of missing values: \n",df.isnull().sum())

df.drop('is_employed_filled', axis=1, inplace=True)

df

# Check if NaN values occur in the same rows
nan_rows = df[df[['housing_type', 'num_vehicles', 'gas_usage']].isna().all(axis=1)]

# Display the rows where all three columns are NaN
print(nan_rows)

# Drop rows where all of the specified columns have NaN at the same time
# Drop rows where all three columns are NaN
df = df.drop(nan_rows.index)


# Check the shape of the DataFrame after dropping rows
print(f"New shape of the dataframe: {df.shape}")

print("Number of missing values: \n",df.isnull().sum())

df[df['recent_move_b'].isna()]

df['recent_move_b'].mode()[0]

# Impute the missing value with the most frequent value (mode)
df['recent_move_b'] = df['recent_move_b'].fillna(df['recent_move_b'].mode()[0])

df['is_employed'] = df['is_employed'].map({'True': True, 'False': False})
df['is_employed'] = df['is_employed'].astype(bool)
df['recent_move_b'] = df['recent_move_b'].map({'T': True, 'F': False})
df['recent_move_b'] = df['recent_move_b'].astype(bool)
# Convert 'object' column to 'string' dtype
df['custid'] = df['custid'].astype('string')
df['sex'] = df['sex'].astype('string')
df['marital_status'] = df['marital_status'].astype('string')
df['housing_type'] = df['housing_type'].astype('string')
df['state_of_res'] = df['state_of_res'].astype('string')

df.dtypes

df

# dropping the ID column: doesn't add much

df.drop('custid', axis=1, inplace=True)
df

print("Some statistics describing the variables.\n")
df.describe()

df['sex'].value_counts().plot(kind='bar')
plt.show()
df['marital_status'].value_counts().plot(kind='bar')
plt.show()
df['housing_type'].value_counts().plot(kind='bar')
plt.show()
plt.figure(figsize=(8,8))
df['state_of_res'].value_counts().plot(kind='bar')
plt.show()
plt.figure(figsize=(8,8))
df['code_column'].value_counts().plot(kind='bar')
plt.show()



#sns.histplot(df['age'], kde=True, bins=30)
sns.boxplot(x='income', data=df)

"""1. Are they young, middle-aged, or seniors?"""

# Define age groups
bins = [0, 40, 60, 100]
labels = ['Young', 'Middle-aged', 'Senior']
df['age_group'] = pd.cut(df['age'], bins=bins, labels=labels)

# Plot age distribution by age group
sns.countplot(x='age_group', data=df)
plt.title('Age Group Distribution')
plt.show()

df.drop('age_group', axis=1, inplace=True)
df

list(df.columns)

xs=list(df.columns)
num_features = len(xs)
# number of rows and columns in the grid
num_rows = (num_features - 1) // 3 + 1
num_cols = min(num_features, 3)
fig, axes = plt.subplots(num_rows, num_cols, figsize=(13, 17))
fig.subplots_adjust(hspace=0.5)  #adjust the vertical space between subplots
for i, x in enumerate(xs):
    row, col = divmod(i, 3) if num_cols == 3 else divmod(i, num_cols)
    sns.histplot(data=df[x], ax=axes[row, col])
    axes[row, col].set_title(x)
#remove empty subplots
for i in range(num_features, num_rows * num_cols):
    row, col = divmod(i, 3) if num_cols == 3 else divmod(i, num_cols)
    fig.delaxes(axes[row, col])
fig.suptitle("Histograms", fontsize=14)
plt.show()

#Separating columns to be visualized
out_cols = list(set(df.nunique()[df.nunique()<6].keys().tolist()
                    + df.select_dtypes(include='object').columns.tolist()))
viz_cols = [x for x in df.columns if x not in out_cols] + ['health_ins']

sns.pairplot(df[viz_cols], diag_kind="kde",hue="health_ins")
plt.suptitle("Pairs plot for every feature, colored by health_ins (blue = No, orange = Yes)", y=1.02, fontsize=22)
plt.show()

df

df_numeric = df.drop(['sex','is_employed','marital_status','housing_type','health_ins', 'state_of_res', 'state_of_res','code_column','recent_move_b'],axis='columns')

correlations = df_numeric.corr()
sns.heatmap(correlations, cmap='coolwarm', linewidths=.5)
plt.suptitle("Correlation Heatmap for Numeric Features", y=1.02)
plt.show()

from PCA_func import PCA_fit,PCA_explained_variance_plots

# Step 1: Standardize the data
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df_numeric)

# Step 2: Perform PCA
pca = PCA()  # PCA with all components
principal_components = pca.fit_transform(df_scaled)

# Create a DataFrame for the principal components (optional)
pca_df = pd.DataFrame(data=principal_components, columns=[f'PC{i+1}' for i in range(principal_components.shape[1])])

# Step 3: Analyze explained variance
print("Explained variance ratio for each component:")
print(pca.explained_variance_ratio_)

# Step 4: Plot cumulative explained variance
plt.figure(figsize=(8, 6))
plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('PCA: Explained Variance')
plt.grid()
plt.show()

# Step 1: Standardize the data
scaler = MinMaxScaler()
df_scaled = scaler.fit_transform(df_numeric)

# Step 2: Perform PCA
pca = PCA()  # PCA with all components
principal_components = pca.fit_transform(df_scaled)

# Create a DataFrame for the principal components (optional)
pca_df = pd.DataFrame(data=principal_components, columns=[f'PC{i+1}' for i in range(principal_components.shape[1])])

# Step 3: Analyze explained variance
print("Explained variance ratio for each component:")
print(pca.explained_variance_ratio_)

# Step 4: Plot cumulative explained variance
plt.figure(figsize=(8, 6))
plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('PCA: Explained Variance')
plt.grid()
plt.show()

df_numeric['health_ins'] = df['health_ins'].map({True: 1, False: 0})

df_numeric['health_ins'] = df['health_ins']

df_numeric.isnull().sum()

# Assume df is your dataset with a target column named 'target'

# Step 1: Separate numerical columns and standardize the data
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df_numeric)

# Step 2: Perform PCA
pca = PCA()
principal_components = pca.fit_transform(df_scaled)

# Step 3: Create a DataFrame for the PCA results
pca_df = pd.DataFrame(data=principal_components, columns=[f'PC{i+1}' for i in range(principal_components.shape[1])])

# Add the target column for coloring
pca_df['health_ins'] = df_numeric['health_ins']
print(pca_df.isnull().sum())
pca_df = pca_df.dropna(subset=['health_ins'])
print(pca_df.isnull().sum())
color_map = {1: 'green', 0: 'red'}
pca_df['color'] = pca_df['health_ins'].map(color_map)
# Step 4: Plot multi-panel scatterplots
plt.figure(figsize=(12, 12))
for i in range(4):  # Loop through the first 10 PCs
    plt.subplot(2, 2, i + 1)  # Create a 4x3 grid for subplots
    for label, color in zip(pca_df['health_ins'].unique(), ['red', 'green']):
        plt.scatter(
            pca_df[f'PC{i+1}'], pca_df[f'PC{i+2}'],  # Plot PC{i+1} vs PC{i+2}
            c=pca_df['color'],
            label=label, alpha=0.6
        )
    plt.title(f'Principal Component {i+1} vs {i+2}')
    plt.xlabel(f'PC{i+1}')
    plt.ylabel(f'PC{i+2}')
    plt.grid()
    if i == 0:  # Add legend to the first subplot
        plt.legend(title='Health ins', loc='upper right')

plt.tight_layout()
plt.suptitle('Principal Component Analysis of Dataset', y=1.02, fontsize=16)
plt.show()

